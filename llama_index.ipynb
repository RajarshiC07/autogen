{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "# Load environment variables from .env file\n",
    "from autogen import  AssistantAgent\n",
    "from autogen.agentchat.contrib.llamaindex_conversable_agent import LLamaIndexConversableAgent\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, GPTVectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.agent import AgentRunner, ReActAgent\n",
    "from llama_index.llms import openai\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.capabilities import teachability\n",
    "from autogen import Cache\n",
    "from autogen.agentchat.contrib.capabilities.text_compressors import LLMLingua\n",
    "from autogen.agentchat.contrib.capabilities.transforms import TextMessageCompressor\n",
    "from autogen.agentchat.contrib.capabilities import transform_messages\n",
    "import autogen\n",
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "import Stemmer\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from llama_index.core.agent import (\n",
    "    StructuredPlannerAgent,\n",
    "    FunctionCallingAgentWorker,\n",
    "    ReActAgentWorker,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.tools import RetrieverTool, FunctionTool\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.selectors import PydanticMultiSelector\n",
    "\n",
    "from llama_index.core.retrievers import RouterRetriever,SummaryIndexRetriever,RecursiveRetriever\n",
    "from llama_index.core import SummaryIndex\n",
    "from huggingface_hub import login\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, FunctionAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "hugging_face_api = os.getenv('HUGGINGFACE_API_KEY')\n",
    "login(hugging_face_api)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_CLOUD_KEY = os.getenv(\"LLAMA_CLOUD_KEY\")\n",
    "\n",
    "#global configs\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "Settings.llm = openai.OpenAI()\n",
    "Settings.embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id de37cff1-09fa-498d-b95e-0078ae7ba5e5\n",
      "Started parsing the file under job_id a67ab1cd-1048-4019-b198-909ad6c82249\n",
      "Started parsing the file under job_id 622ffd98-f321-415b-99ee-b885aea5aadb\n",
      "Started parsing the file under job_id 65d6e6a5-c89c-440b-8a27-ddc80e9c69fa\n",
      "Started parsing the file under job_id d0674ee8-833e-43b7-970b-9eb68a71cc2b\n",
      "[Document(id_='674d4d40-d166-492c-9daf-1a27aecd318d', embedding=None, metadata={'file_path': 'd:\\\\docs\\\\LLM_RAG_structureddata\\\\autogen\\\\Data_repo\\\\Kamya_Resume.pdf', 'file_name': 'Kamya_Resume.pdf', 'file_type': 'application/pdf', 'file_size': 437812, 'creation_date': '2025-02-25', 'last_modified_date': '2024-12-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Kamya\\n\\ncontact: +91-9268303027\\n\\nLocation: RZ -630A, Lane 27e-1, Sadh Nagar - 2, Palam Colony, New Delhi -110045\\n\\nLinkedin: Linkedin ,   Email: kamya.sarkania@gmail.com\\n\\n# SUMMARY\\n\\nI am a motivated professional with strong analytical and investigative skills, demonstrated during my role as a CDD Exits Analyst at NatWest. I excel in process improvement, data analysis, and effective communication. A collaborative team player, I have consistently met KPIs and delivered results within SLAs while contributing to team success.\\n\\n# EDUCATION\\n\\n|Program|Institute|Completion Year|\\n|---|---|---|\\n|B.Com|Maitreyi College|2017-2020|\\n|XII (CBSE Board)|Indira Ideal Sr Sec School|2017|\\n|X (CBSE Board)|Indira Ideal Sr Sec School|2015|\\n\\n# SKILLS\\n\\n# Core Skills:\\n\\nAnalytical skills, due diligence, MS Office, attention to detail, investigation skills, customer support, brainstorming, research, problem solving, data collection, data analysis\\n\\n# Communication Skills:\\n\\nConfidence, excellent written & verbal ability, active listening, clarity, feedback, attentiveness, inquisitiveness\\n\\n# Leadership Skills:\\n\\nEmpathy, creativity, reliability, time-effective deliverance of task, comfortability\\n\\n# Professional Experience\\n\\n# NATWEST, — CDD EXITS ANALYST\\n\\nOctober 2021 – June 2023\\n\\n- Conducted detailed investigations of customer data to determine eligibility for exits, ensuring compliance with guidelines and maintaining accuracy.\\n- Demonstrated strong process mastery, achieving full scores in multiple knowledge assessments and excelling across various sub-processes.\\n- Consistently delivered high-quality work, meeting 100% of KPIs and SLA deadlines, earning recognition from management.\\n- Contributed to process optimization by presenting actionable ideas and implementing time-saving strategies, improving team efficiency by 15%.\\n- Played a key role in team engagement by organizing activities, mentoring new members, and fostering collaboration.\\n- Gained confidence in presenting ideas effectively, both individually and in team discussions, enhancing communication skills.\\n- Built strong professional relationships, cultivating a supportive and positive work environment.\\n- Selected to engage with senior leadership as a trusted and recognized contributor, demonstrating expertise and reliability.\\n\\n# ATC ENGINEERING, NOIDA— ACCOUNTS INTERN\\n\\nMarch 2018 – May 2018\\n\\n- Gained hands-on experience as an intern in the Accounts Department, enhancing practical knowledge of financial operations.\\n- Learned the basics and practical application of Tally for accounting tasks.\\n- Assisted in company projects, contributing to their successful execution and timely completion.\\n- Consistently delivered work with responsibility and adherence to deadlines.\\n- Earned recognition for reliability and commitment to maintaining high-quality standards.\\n\\n# CERTIFICATIONS\\n\\n- World Class Skill Centre - Finance executive Course - 2017-2018\\n- British Council - Spoken English Certification - 2017\\n\\n# EXTRA-CURRICULAR ACTIVITIES\\n\\n- Volunteered In Medical Health Check Up Camps In And Out Station\\n- Provided Free Education To Underprivileged Kids\\n- National Taekwondo Player And Delhi Olympic Medals\\n\\n# LANGUAGES\\n\\n- Hindi (Mother Tongue)\\n- English (Fluent)\\n- German (Elementary Level)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0e9b14ba-2b1e-490f-9a27-49a05acc8ed6', embedding=None, metadata={'file_path': 'd:\\\\docs\\\\LLM_RAG_structureddata\\\\autogen\\\\Data_repo\\\\Mrittika_Das_resume.pdf', 'file_name': 'Mrittika_Das_resume.pdf', 'file_type': 'application/pdf', 'file_size': 466610, 'creation_date': '2025-02-25', 'last_modified_date': '2024-03-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# KOLKATA, INDIA\\n\\nMrittika Das\\n\\n+91 7596832189\\n\\ndasmrittika6@gmail.com\\n\\nLinkedIn\\n\\n# SUMMARY\\n\\nSoftware Engineer with over 2.6 years of experience. Experienced working with Ping Products. Design, Development, Testing and implementation of security applications using PingFederate, PingAccess, PingRisk, PingOne, Davinci and PingID. In-depth knowledge of IAM modules like SSO, SAML, OAuth.\\n\\n# WORK EXPERIENCE\\n\\n# Accenture\\n\\nAugust 2021-Present\\n\\nApplication Development Analyst (Role: PING ENGINEER) - Kolkata, India\\n\\n# Projects\\n\\n# Kaiser Permanente:\\n\\nDescription: This Project focuses on management of identities, single sign on and security for enterprise applications by integrating client application with PingFederate.\\n\\n# Responsibilities:\\n\\n- Proficient in Protocols: SAML, Web SSO, OAuth, and application onboarding.\\n- Experience in managing SP connections for deploying new applications.\\n- Hands on experience in configuring and testing Policies, Adapters, Selectors and Fragments.\\n- Experience in upgradation of PingFederate application SAML & OAuth.\\n- Hands-on experience on Multi-Factor Authentication by integrating PingID with the existing flow.\\n- Experience in configuring and integrating PingRisk with PingFederate policy for better security experience.\\n- Configured PingFedSSO with PingAccess to fortify application protection.\\n\\n# LOKHAI:\\n\\nDescription: This Project focuses on implementing password-less login via biometric for Windows Hello for Business, Touch ID AND FACE ID for client managed devices.\\n\\n# Responsibilities:\\n\\n- Worked on Policy creation for FIDO based login integrating PingFederate and Ping Access.\\n- Dynamic HTML page creation for password-less login using Touch ID /Face ID based upon device.\\n- Configured Policy to allow only internal user to register for Touch ID/Face ID.\\n- Worked on Solution and Architectural Document for this project.\\n\\n# DAVINCI:\\n\\nDescription: This Project concentrates on crafting flows within Davinci, intending to function akin to the current PingFederate policy.\\n\\n# Responsibilities:\\n\\n- Worked on flow creation for FIDO based login integrating PingFederate and PingOne Davinci.\\n- Designed flow for FIDO based login using PingOne MFA connector.\\n- Transferred intricate policies from PingFederate to Davinci flow for enhanced efficiency.\\n- Configured SAML and OAuth application in PingOne.\\n- Configured HTML pages for PingOne flows.\\n\\n# TECHNICAL SKILLS\\n\\n- JAVA\\n- PingFederate\\n- PingAccess\\n- PingOne\\n- Davinci\\n- HTML\\n- CSS\\n\\n# EDUCATION\\n\\nBachelor of Technology in Electronics and Communication Engineering. - Kolkata, India\\n\\nRCC Institution of Information Technology - Graduation: 06/2021\\n\\nDGPA- 8.96\\n\\n# TRAININGS & CERTIFICATION\\n\\n- Microsoft Certified: Azure AI Fundamentals.\\n\\n# ACHIEVEMENTS\\n\\n- FY’23 ATCI NA Pinnacle Award Winner.\\n- ACE Award Q1, FY24 Individual award in the Skills Star Award category.\\n\\n# LANGUAGES\\n\\n- ENGLISH\\n- BENGALI\\n- HINDI', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e7c19133-cd92-4a86-af76-2d6fec05b486', embedding=None, metadata={'file_path': 'd:\\\\docs\\\\LLM_RAG_structureddata\\\\autogen\\\\Data_repo\\\\Oindrila_Chakravarty_Resume.pdf', 'file_name': 'Oindrila_Chakravarty_Resume.pdf', 'file_type': 'application/pdf', 'file_size': 72695, 'creation_date': '2025-02-25', 'last_modified_date': '2024-06-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Oindrila Chakravarty\\n\\n# Data Engineer\\n\\nWorking as an AWS Data engineer for a Financial Services client to deal with automations and draw insights from data and perform transformations on various data to get desired data output.\\n\\n# Contact\\n\\nPhone: +91 8981037204\\n\\nEmail: chakravartyoindrila@gmail.com\\n\\nAddress: Udayrajpur, Arabinda Sarani, Madhyamgram, Kolkata 700129\\n\\n# Experience\\n\\nCapgemini\\n\\nFeb 2021 - Associate Consultant\\n\\n- Creation of build and deploy plans for CI/CD pipelines\\n- Creation of repositories through Bitbucket and deployment through Bamboo\\n- Creation of AWS IAM roles for restricted access\\n- Perform ETL tasks in AWS Glue\\n- Execute Pyspark transformation scripts\\n- Data Ingestion to AWS S3\\n- Automation to trigger AWS Glue job through AWS Lambda\\n- Creation of event-based triggers for data ingestion\\n- Creation of datafeed dashboard in Splunk to monitor daily activities of AWS Glue job\\n- Parsing of file format such as csv, excel and parquet and writing them to AWS S3\\n- Creation of optimised queries through AWS Athena\\n- Creation of unit test cases to validate data\\n- Creation of optimised Hive queries\\n- Executing optimised sql scripts through Athena query executor\\n\\n# Education\\n\\n2020 - B.Tech, Applied Electronics and Instrumentation Engineering\\n\\nHeritage Institute of Technology\\n\\n2016 - ISC, Science\\n\\nAuxilium Convent School\\n\\n2014 - ICSE, Science\\n\\nAuxilium Convent School\\n\\n# Skills\\n\\n- Pyspark\\n- AWS SNS\\n- Python\\n- AWS Cloudformation\\n- AWS Lambda\\n- AWS IAM\\n- AWS Glue\\n- Bitbucket\\n- AWS S3\\n- Bamboo\\n- AWS Athena\\n- Splunk\\n- AWS CloudWatch\\n- Hive\\n- SQL\\n\\n# Language\\n\\n- English\\n- Bengali\\n- Hindi\\n\\n# Certification\\n\\n- Python Functions, Files, and Dictionaries - University of Michigan, Coursera\\n- Agile Software Development - University of Minnesota, Coursera\\n- Big Data Essentials: HDFS, MapReduce and Spark RDD - Yandex, Coursera\\n- The Data Scientist’s Toolbox - John Hopkins University, Coursera\\n\\n# Projects\\n\\nFinger Oriented Signalling System (2019 - 2020)\\n\\nTo enable recognition of hand gestures for disabled patients using sensors\\n\\nTechnologies: C, Flex sensor, LED, ATMEGA8L\\n\\nAnalysis of Football Dataset (2021)\\n\\nExtraction of top 10 players from various sections of a Football team\\n\\nTechnologies: HDFS, Hive, Talend', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2962cfb8-2e15-4b09-922c-aa9e2e59d05d', embedding=None, metadata={'file_path': 'd:\\\\docs\\\\LLM_RAG_structureddata\\\\autogen\\\\Data_repo\\\\Rajarshi_Chatterjee_Resume.pdf', 'file_name': 'Rajarshi_Chatterjee_Resume.pdf', 'file_type': 'application/pdf', 'file_size': 383367, 'creation_date': '2025-02-25', 'last_modified_date': '2024-10-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# RAJARSHI CHATTERJEE\\n\\ncontact: +91-9874564953 Location: Kolkata\\n\\nLinkedin: Linkedin ,   Email: rajarshi2707@gmail.com  ,      Github:   Git-Link\\n\\n# SUMMARY\\n\\nWith experience in data engineering and data science, I specialize in creating optimized solutions and sustainable frameworks using Python and AWS. My expertise extends to building generative AI applications, leveraging LangChain and LlamaIndex to develop intuitive interfaces for querying financial reports. I am passionate about innovation, consistently delivering solutions that enhance efficiency and drive business success. My problem-solving skills are complemented by a strong ability to integrate and streamline complex data systems.\\n\\n# EDUCATION\\n\\n|Program|Institute|CGPA / %age|Completion Year|\\n|---|---|---|---|\\n|B.Tech|Meghnad Saha Institute of Technology|9.12|2017-2021|\\n|XII (ISC Board)|W.W.A. Cossipore English School|92.2%|2017|\\n|X (ICSE Board)|W.W.A. Cossipore English School|94.75%|2015|\\n\\n# SKILLS\\n\\nTechnical: Python and its libraries like Pandas, Pyspark, SKlearn, Lang-chain, Streamlit, tkinter, spacy, cx_freeze, Flutter; AWS services like S3, Glue, IAM, Cloud formations, Aurora, Redshift, Crawler, DynamoDB, Athena; Databases: Oracle, Aurora, Redshift, DynamoDB, Pinecone, Milvus Ziliz; Understanding of Data Structures and their implementation. Avid coder at leetcode; Understanding of data storage, governance and various data mining architectural models and SQL; Skilled with the utilization of Large Language Models for building customized AI apps.\\n\\nFunctional: Effective Communication and Stakeholder Engagement Skills; Proactive Problem-Solving and driven to continuous Learning; Skilled at Strategic Team Leadership and Task Management.\\n\\n# PROJECTS\\n\\n# Client: US Based Private Equity Firm\\n\\nOctober 2022 – till Date\\n\\n# Project 1\\n\\nResponsibilities\\n\\n- Analyzed and categorized reports into Simple, Medium, Complex, and Super Complex levels, ensuring appropriate handling and optimization based on their intricacies.\\n- Developed a Python-based automation tool for mapping report attributes from the Cognos Framework Manager, enabling data export to Excel sheets and reducing manual intervention by 60%.\\n- Engineered ETL infrastructure using CloudFormation templates and led the development of an ETL framework with AWS Glue to generate reports within the AWS Cloud ecosystem utilizing shared resources for optimization.\\n- Implemented AWS Lambda and Storage Gateway for on-demand report generation and created a Python-based stand-alone UI tool for user-friendly report triggering, integrating it with cloud services to enable automatic updates.\\n\\n# Innovation Wing\\n\\nJuly 2023 – August 2023\\n\\n# Project-2\\n\\n- Developed a LangChain-based financial report retrieval system, employing vector databases, prompts, and agents to deliver customized search results, enhanced by conversation memory for seamless user interaction.\\n- Designed a self-sustaining pipeline for scheduled data mining from financial reports, incorporating preprocessing and hallucination mitigation with tailored fact-checking, reducing analysis time by 50%.\\n\\n# Personal Projects\\n\\nJuly 2021 - continued\\n\\n# Project 1\\n\\n- Developed a Retrieval-Augmented Generation (RAG) chatbot using LangChain and Llamaindex, leveraging open-source LLMs such as HuggingFaceHub and Gemini AI, along with VectorDB Zilliz. The user interface was built with Flutter and packaged with PyInstaller to efficiently retrieve candidate information from uploaded resumes.\\n\\n# CERTIFICATIONS\\n\\n- AWS Developer Associate\\n- Azure AI Fundamentals\\n\\n# EXTRA-CURRICULAR ACTIVITIES\\n\\n- Painting (National and State Award winner)\\n- Gaming\\n- Debates and Tech podcasts\\n- Metaverse Exploration', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='50f4c3bf-034f-4c0e-a3ab-ccc0e2d08325', embedding=None, metadata={'file_path': 'd:\\\\docs\\\\LLM_RAG_structureddata\\\\autogen\\\\Data_repo\\\\Richa_Gupta_AWS_Eng_Resume.pdf', 'file_name': 'Richa_Gupta_AWS_Eng_Resume.pdf', 'file_type': 'application/pdf', 'file_size': 78271, 'creation_date': '2025-02-25', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Richa Gupta\\n\\n# AWS Cloud Engineer\\n\\nricha2896.rg@gmail.com &nbsp; +91 9878203112 &nbsp; LinkedIn\\n\\n# PROFILE\\n\\nSeasoned AWS Cloud Engineer with 5+ years orchestrating advanced cloud solutions. Passionate about automation, DevOps, and system administration to boost business agility. Recognized for creating an automated AWS Cloud ETL framework, achieving 70% improvement in job monitoring and cost efficiency. Committed to delivering transformative solutions for business growth in the digital era.\\n\\n# PROFESSIONAL EXPERIENCE\\n\\n# Capgemini India Pvt. Ltd., Consultant\\n\\nJul 2019 – present | Noida, Uttar Pradesh, India\\n\\n# EDUCATION\\n\\nB.Tech, National Institute of Technology\\n\\n2015 – 2019 | Jalandhar, India\\n\\nCGPA: 7.95\\n\\nXII (CBSE), Sri Guru Harkrishan Public School\\n\\n2013 – 2015 | Amritsar, India\\n\\nScored: 92.20%\\n\\nX (ICSE), St. Francis School\\n\\n2013 | Amritsar, India\\n\\nScored: 94.70%\\n\\n# SKILLS\\n\\nAWS Services — S3, KMS, Glue, Athena, IAM, Lambda, EKS, EC2, Patch Manager, Lake Formation, CloudWatch, Secrets Manager, SNS, SQS, CloudTrail, SageMaker, API Gateway, AWS transfer family SFTP, Cost and Billing, CFT\\n\\nProgramming — Python, SQL\\n\\nDatabases — MySQL, DBeaver\\n\\nCI/CD — BitBucket, Jenkins, Bamboo, GitHub\\n\\nBig Data Tools — PySpark, Hive, Hadoop, Sqoop, Talend\\n\\nOther Tools & Tech. — Terraform, JIRA, New Relic Dashboard, GenAI\\n\\nSoft skills — Effective communication and teamwork | Positive attitude towards learning | Critical thinking\\n\\n# CERTIFICATIONS\\n\\nTechnical Architect Level 0 certification\\n\\nAWS Certified Solutions Architect Associate (Pursuing)\\n\\n# PROJECT DETAILS\\n\\n# COX Communications, AWS Cloud Engineer\\n\\nOct 2023 – present\\n\\n- Spearhead automation initiatives using AWS, Python, and SQL, reducing processing time by 30%.\\n- Collaborate with the architecture team to design and refine COX project frameworks.\\n- Engagement with clients to understand requirements, translating them into AWS-based strategies and integrating them into new or existing infrastructure.\\n- Ensure smooth deployments by managing access, permissions, and resolving errors/issues, achieving a 95% success rate in deployments.\\n- Manage billing and cost optimization, resolving anomalies, saving 10% in monthly operational costs.\\n- Develop documentation and conduct training, enhancing team skills in AWS and Python solutions.\\n- Lead an offshore team of four, integrating technical and administrative responsibilities to ensure efficient project execution.\\n\\n# Vanguard (US), AWS Data Engineer\\n\\nNov 2021 – Sep 2023\\n\\n- Built ETL pipelines using AWS services and CI/CD pipelines with Bitbucket and Bamboo to generate reports. Delivered reverse engineering reports (Simple to Super Complex) with IBM Cognos Framework Manager.\\n- Performed Python transformations, including ranking and segmentation, and re-calibrated ML models to reduce effort by 80%.\\n- Automated value-based segmentation reports using AWS Lambda and Glue.\\n\\n# Resource Manager, Staffing & Operations\\n\\nJul 2020 – Oct 2021\\n\\n- Optimized resources for HSBC (UK) and Vanguard (US) accounts, collaborated with BU Delivery Managers and technical leads to gather requirements.\\n- Forecasted business futures using historical reports and trends and mentored 100+ associates, managing workforce and virtual onboarding/offboarding.\\n\\n# ACHIEVEMENTS\\n\\nRISING STAR award 2024 | SPECIAL CONTRIBUTION award 2023 | HELPING HAND award 2023 | Best Agile Player Team award 2021 and 2020', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and process documents (ensure you have a `documents/` folder)\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_KEY,  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_dir=r\"Data_repo\", file_extractor=file_extractor).load_data()\n",
    "\n",
    "# Create a vector index\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamya_Resume\n",
      "MrittikaDas_resume\n",
      "Oindrila_Chakravarty_Resume\n",
      "Rajarshi_Chatterjee_Resume\n",
      "Richa_Gupta_AWS_Eng_Resume\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(str(doc.metadata['file_path'].split(\"\\\\\")[-1]).split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating nodes, agent, indexes, retrievers, query engines, agents for each docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = []\n",
    "query_engine_tools_list = []\n",
    "initial_name = None\n",
    "for doc in documents:\n",
    "    splitter = SentenceSplitter(chunk_size=200, chunk_overlap=70)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_indexes = GPTVectorStoreIndex(nodes)\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_indexes, similarity_top_k=3)\n",
    "    vector_query_engine = RetrieverQueryEngine(\n",
    "                retriever=vector_retriever,\n",
    "            )\n",
    "\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    summary_index_query_engine = summary_index.as_query_engine()\n",
    "\n",
    "    memory = ChatMemoryBuffer(token_limit=1024)\n",
    "\n",
    "\n",
    "    query_engine_tools = [QueryEngineTool(query_engine=vector_query_engine,\n",
    "                                        metadata=ToolMetadata(\n",
    "                                            name='vector_tool', \n",
    "                                            description=f\"This tool is used to retrieve detailed data for documents related to {str(doc.metadata['file_path'].split(\"\\\\\")[-1])}. For example years of experience, tech stacks, companies worked for, project details etc.\")\n",
    "                                        ),\n",
    "                        QueryEngineTool(query_engine=summary_index_query_engine,\n",
    "                                        metadata=ToolMetadata(\n",
    "                                            name='summary_tool', \n",
    "                                            description=f\"This tool is used to retrieve summarized info from documents related to {str(doc.metadata['file_path'].split(\"\\\\\")[-1])}. For example name, overall experience, introduction etc.\")\n",
    "\n",
    "                                        )]\n",
    "    \n",
    "    review_agent = FunctionAgent(\n",
    "                name=f\"{str(doc.metadata['file_path'].split(\"\\\\\")[-1]).split('.')[0]}_Agent\",\n",
    "                description=f\"Reviews resume to provide information regarding {str(doc.metadata['file_path'].split(\"\\\\\")[-1]).split('.')[0]}\",\n",
    "                system_prompt=\"You are a meticulous recruiter who scans through resumes to provide details...\",\n",
    "                tools=query_engine_tools_list\n",
    "            ) \n",
    "    initial_name = f\"{str(doc.metadata['file_path'].split(\"\\\\\")[-1]).split('.')[0]}_Agent\"\n",
    "    \n",
    "    query_engine_tools_list.extend(query_engine_tools)\n",
    "    agent_list.append(review_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AgentWorkFlow with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    query_engine_tools_list,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful assistant that can scan through documents.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Context for workflow memory and state awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = Context(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context (ctx) can be stored for the duration of the conversation session. Once the conversation session ends or times out, the context is typically cleared. If you have specific requirements for storing context beyond the session, you may need to implement custom storage and retrieval mechanisms.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    user_msg=\"how long can the ctx be stored?\", ctx=ctx\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving context to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
    "import json\n",
    "with open(\"local_context.json\", \"w+\") as file:\n",
    "    json.dump(ctx_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the stored context and reusing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your previous question was \"do you remember the names of the candidates?\"\n"
     ]
    }
   ],
   "source": [
    "restored_ctx = Context.from_dict(\n",
    "    workflow, ctx_dict, serializer=JsonSerializer()\n",
    ")\n",
    "\n",
    "response3 = await workflow.run(user_msg=\"What was my previous question?\", ctx=restored_ctx)\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Agentworkflow with agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiator_agent():\n",
    "    # \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return \"Initiating\"\n",
    "\n",
    "\n",
    "initiator_tool = FunctionTool.from_defaults(fn=initiator_agent)\n",
    "\n",
    "# initialize ReAct agent\n",
    "agent = FunctionAgent(\n",
    "    name=\"InitiatorAgent\",\n",
    "    description=\"This just bypasses the tasks to the required agents as per the queries from the user.\",\n",
    "    system_prompt=\"You are an expert orchestrator...\",\n",
    "    tools=[initiator_tool]\n",
    ")\n",
    "\n",
    "agent_list.append(agent)\n",
    "\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=agent_list,\n",
    "    root_agent='InitiatorAgent',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have initiated the process to gather the names of all the candidates. Let me retrieve the information for you.\n",
      "I have retrieved the names of all the candidates. Here are the names:\n",
      "\n",
      "1. Kamya\n",
      "2. Mrittika Das\n",
      "3. Oindrila Chakravarty\n",
      "4. Rajarshi Chatterjee\n",
      "5. Richa Gupta\n",
      "\n",
      "If you need more information about any specific candidate, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = await agent_workflow.run(\n",
    "    user_msg=\"Tell me the names of all the candidates?\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good AgentRunner usage. Preferred for simple version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The names of all the candidates are:\\n1. Kamya\\n2. Mrittika Das\\n3. Oindrila Chakravarty\\n4. Rajarshi Chatterjee'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent_runner = AgentRunner.from_llm(query_engine_tools_list,)\n",
    "agent_openAI = OpenAIAgent.from_tools(query_engine_tools,\n",
    "                                    verbose=True,\n",
    "                                    memory=memory,\n",
    "                                    system_prompt=f\"You are a specialized agent designed to provide information for document. You must always use any one of the tools if question is regarding this user or a generic question. DO NOT RELY ON PAST KNOWLEDGE.\")\n",
    "\n",
    "\n",
    "query = 'Tell me the names of all the candidates?'\n",
    "response = agent_runner.query(query)\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating llamaindex with langchain.\n",
    "Llamaindex for the tools.\n",
    "Langchain for the agents and chat memory history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_tools = [tools.to_langchain_tool() for tools in query_engine_tools_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# Get the prompt to use - you can modify this!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\docs\\LLM_RAG_structureddata\\autogen\\.venv\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "print(prompt.messages)\n",
    "agent = create_tool_calling_agent(llm, langchain_tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=langchain_tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Kamya_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKamya is a motivated professional with strong analytical and investigative skills, specializing as a CDD Exits Analyst at NatWest. She excels in process improvement, data analysis, and effective communication. Kamya is known for consistently meeting KPIs, delivering results within SLAs, and contributing to team success. She has demonstrated expertise in conducting detailed investigations of customer data, ensuring compliance with guidelines, and maintaining accuracy. Additionally, Kamya has actively contributed to process optimization, team engagement, and building strong professional relationships within the organization.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Mrittika_Das_resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mMrittika Das is a Software Engineer with over 2.6 years of experience, specializing in working with Ping Products. She has expertise in IAM modules like SSO, SAML, OAuth, and has worked on projects focusing on identity management, single sign-on, and security for enterprise applications. Mrittika has hands-on experience in configuring and testing policies, adapters, selectors, and fragments, as well as in managing SP connections for deploying new applications. Additionally, she has experience in integrating PingID, PingRisk, and PingOne for enhanced security measures.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Oindrila_Chakravarty_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mOindrila Chakravarty is an AWS Data Engineer working with a Financial Services client to automate processes, draw insights from data, and perform data transformations. She has experience in creating build and deploy plans for CI/CD pipelines, managing repositories, and executing ETL tasks in AWS Glue. Oindrila has expertise in technologies like Pyspark, AWS SNS, Python, AWS Cloudformation, AWS Lambda, AWS IAM, and more. She holds certifications in Python Functions, Files, and Dictionaries, Agile Software Development, and Big Data Essentials. Additionally, she has worked on projects like the Finger Oriented Signalling System and Analysis of Football Dataset, showcasing her technical skills and experience in data engineering.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Rajarshi_Chatterjee_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRajarshi Chatterjee is a professional with expertise in building generative AI applications and creating sustainable frameworks using Python and AWS. He has a strong background in data engineering and data science, focusing on developing innovative solutions to enhance efficiency and drive business success. Rajarshi has worked on projects involving financial report retrieval systems, ETL infrastructure development, and automation tools. He is skilled in Python, AWS services, data structures, and large language models for AI applications. Additionally, he excels in effective communication, stakeholder engagement, problem-solving, and strategic team leadership.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Richa_Gupta_AWS_Eng_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRicha Gupta is a seasoned AWS Cloud Engineer with a strong background in orchestrating advanced cloud solutions. She has over 5 years of experience and is passionate about automation, DevOps, and system administration to enhance business agility. Richa is recognized for creating an automated AWS Cloud ETL framework that significantly improved job monitoring and cost efficiency. She holds a B.Tech degree from the National Institute of Technology and has expertise in various AWS services like S3, Glue, Athena, IAM, Lambda, among others. Richa has worked on projects at Capgemini and Vanguard, showcasing her skills in ETL pipeline building, automation, and client engagement. She has received several awards for her contributions, including the RISING STAR award in 2024 and the SPECIAL CONTRIBUTION award in 2023.\u001b[0m\u001b[32;1m\u001b[1;3mThe names of the candidates are:\n",
      "1. Kamya\n",
      "2. Mrittika Das\n",
      "3. Oindrila Chakravarty\n",
      "4. Rajarshi Chatterjee\n",
      "5. Richa Gupta\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The names of the candidates are:\\n1. Kamya\\n2. Mrittika Das\\n3. Oindrila Chakravarty\\n4. Rajarshi Chatterjee\\n5. Richa Gupta'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Can you tell me the name of all the candidates?\"})['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain agent with chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Kamya_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKamya is a motivated professional with strong analytical and investigative skills, specializing as a CDD Exits Analyst at NatWest. She excels in process improvement, data analysis, and effective communication. Kamya is known for consistently meeting KPIs and delivering results within SLAs while actively contributing to team success. Additionally, she is recognized for her collaborative approach, presenting actionable ideas for process optimization and implementing time-saving strategies to enhance team efficiency. Kamya also plays a key role in team engagement by organizing activities, mentoring new members, and fostering collaboration, showcasing her leadership skills and dedication to a supportive work environment.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Mrittika_Das_resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mMrittika Das is a Software Engineer with over 2.6 years of experience, specializing in working with Ping Products. She has expertise in IAM modules like SSO, SAML, OAuth, and has worked on projects focusing on identity management, single sign-on, and security for enterprise applications. Mrittika has hands-on experience in configuring and testing policies, adapters, selectors, and fragments, as well as in integrating various Ping products like PingFederate, PingAccess, PingOne, Davinci, and PingID. Additionally, she has worked on projects implementing password-less logins via biometric methods and crafting flows within Davinci.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Oindrila_Chakravarty_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mOindrila Chakravarty is an AWS Data Engineer working on automations and data insights for a Financial Services client. She has experience in creating ETL frameworks, executing transformations on data, and optimizing data output. Oindrila has expertise in technologies like Pyspark, AWS SNS, Python, AWS Cloudformation, AWS Lambda, AWS IAM, and more. She holds certifications in Python Functions, Files, and Dictionaries, Agile Software Development, and Big Data Essentials. Additionally, she has worked on projects like the Finger Oriented Signalling System and Analysis of Football Dataset.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Rajarshi_Chatterjee_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRajarshi Chatterjee is a professional with expertise in building generative AI applications and creating sustainable frameworks using Python and AWS. He has experience in developing innovative solutions that enhance efficiency and contribute to business success. Rajarshi has strong problem-solving skills and excels in integrating and streamlining complex data systems.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summary_tool` with `{'input': 'Richa_Gupta_AWS_Eng_Resume.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRicha Gupta is a seasoned AWS Cloud Engineer with a strong background in orchestrating advanced cloud solutions. She has a proven track record of creating automated AWS Cloud ETL frameworks, leading to significant improvements in job monitoring and cost efficiency. Richa's expertise lies in automation, DevOps, and system administration, all aimed at enhancing business agility. She has worked on various projects where she collaborated with teams to design and refine project frameworks, engage with clients to translate requirements into AWS-based strategies, and ensure smooth deployments with a high success rate. Richa's skills include proficiency in AWS services like S3, Glue, Athena, IAM, Lambda, as well as programming in Python and SQL. She holds certifications such as Technical Architect Level 0 and is pursuing the AWS Certified Solutions Architect Associate certification.\u001b[0m\u001b[32;1m\u001b[1;3mBased on the summaries of the candidates, here is a brief overview of their backgrounds:\n",
      "\n",
      "1. **Kamya** - Motivated professional with strong analytical and investigative skills, specializing as a CDD Exits Analyst at NatWest. Excels in process improvement, data analysis, and effective communication. Recognized for meeting KPIs, delivering results within SLAs, and contributing to team success.\n",
      "\n",
      "2. **Mrittika Das** - Software Engineer with expertise in IAM modules like SSO, SAML, OAuth, and projects focusing on identity management and security for enterprise applications. Hands-on experience in configuring policies, adapters, and integrating Ping products.\n",
      "\n",
      "3. **Oindrila Chakravarty** - AWS Data Engineer working on automations, data insights, and ETL frameworks for a Financial Services client. Expertise in technologies like Pyspark, AWS SNS, Python, AWS Cloudformation, and more. Holds certifications in Python, Agile Software Development, and Big Data Essentials.\n",
      "\n",
      "4. **Rajarshi Chatterjee** - Professional with expertise in building generative AI applications and sustainable frameworks using Python and AWS. Strong problem-solving skills and experience in developing innovative solutions.\n",
      "\n",
      "5. **Richa Gupta** - Seasoned AWS Cloud Engineer with a background in orchestrating advanced cloud solutions, automation, DevOps, and system administration. Proficient in AWS services like S3, Glue, Athena, IAM, Lambda, Python, and SQL.\n",
      "\n",
      "Based on the provided summaries, **Rajarshi Chatterjee** appears to be the candidate best suited for a role as an AI Engineer due to his expertise in building generative AI applications and sustainable frameworks using Python and AWS.\n",
      "\n",
      "If you need more detailed information or have any specific criteria in mind for selecting the best-suited candidate, feel free to let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me the name of teh candidate best suited?',\n",
       " 'chat_history': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='can you tell me the names of all the candidates?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The names of the candidates are:\\n1. Kamya\\n2. Mrittika Das\\n3. Oindrila Chakravarty\\n4. Rajarshi Chatterjee\\n5. Richa Gupta\\n\\nIs there anything specific you would like to know about any of these candidates?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='who is most suited for a role of AI Engineer?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Based on the summaries of the candidates, here is a brief overview of their backgrounds:\\n\\n1. **Kamya** - Strong analytical and investigative skills, process improvement expertise, and effective communication skills.\\n2. **Mrittika Das** - Specializes in IAM modules like SSO, SAML, OAuth, and has experience in identity management and security for enterprise applications.\\n3. **Oindrila Chakravarty** - AWS Data Engineer with experience in automations, data insights, transformations, and technologies like Pyspark, AWS Glue, Python, etc.\\n4. **Rajarshi Chatterjee** - Expertise in building generative AI applications and sustainable frameworks using Python and AWS.\\n5. **Richa Gupta** - Seasoned AWS Cloud Engineer with expertise in orchestrating advanced cloud solutions, automation, DevOps, and system administration.\\n\\nConsidering the role of an AI Engineer, **Rajarshi Chatterjee** seems to be the most suited candidate based on his expertise in building generative AI applications and frameworks using Python and AWS. His experience in developing innovative solutions aligns well with the requirements of an AI Engineer role.\\n\\nIf you need more detailed information or have any specific criteria in mind for the AI Engineer role, feel free to let me know!', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Based on the summaries of the candidates, here is a brief overview of their backgrounds:\\n\\n1. **Kamya** - Motivated professional with strong analytical and investigative skills, specializing as a CDD Exits Analyst at NatWest. Excels in process improvement, data analysis, and effective communication. Recognized for meeting KPIs, delivering results within SLAs, and contributing to team success.\\n\\n2. **Mrittika Das** - Software Engineer with expertise in IAM modules like SSO, SAML, OAuth, and projects focusing on identity management and security for enterprise applications. Hands-on experience in configuring policies, adapters, and integrating Ping products.\\n\\n3. **Oindrila Chakravarty** - AWS Data Engineer working on automations, data insights, and ETL frameworks for a Financial Services client. Expertise in technologies like Pyspark, AWS SNS, Python, AWS Cloudformation, and more. Holds certifications in Python, Agile Software Development, and Big Data Essentials.\\n\\n4. **Rajarshi Chatterjee** - Professional with expertise in building generative AI applications and sustainable frameworks using Python and AWS. Strong problem-solving skills and experience in developing innovative solutions.\\n\\n5. **Richa Gupta** - Seasoned AWS Cloud Engineer with a background in orchestrating advanced cloud solutions, automation, DevOps, and system administration. Proficient in AWS services like S3, Glue, Athena, IAM, Lambda, Python, and SQL.\\n\\nBased on the provided summaries, **Rajarshi Chatterjee** appears to be the candidate best suited for a role as an AI Engineer due to his expertise in building generative AI applications and sustainable frameworks using Python and AWS.\\n\\nIf you need more detailed information or have any specific criteria in mind for selecting the best-suited candidate, feel free to let me know!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Tell me the name of teh candidate best suited?\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
